@article{basharin,
author = {Basharin, G. P.},
title = {On a Statistical Estimate for the Entropy of a Sequence of Independent Random Variables},
journal = {Theory of Probability \& Its Applications},
volume = {4},
number = {3},
pages = {333-336},
year = {1959},
doi = {10.1137/1104033},
URL = {https://doi.org/10.1137/1104033},
eprint = {https://doi.org/10.1137/1104033},
}


@INPROCEEDINGS{bohme:fse:2020,
 author = {B{\"o}hme, Marcel and Man{\`e}s, Valentin and Cha, Sang Kil},
 title = {Boosting Fuzzer Efficiency: An Information Theoretic Perspective},
 booktitle = {Proceedings of the 14th Joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
 series = {ESEC/FSE},
 year = {2020},
 pages = {970-981},
 numpages = {12},
 doi = {10.1145/3368089.3409748}
}

@article{blackwell2023hyperfuzzing,
  title={Hyperfuzzing: black-box security hypertesting with a grey-box fuzzer},
  author={Blackwell, Daniel and Becker, Ingolf and Clark, David},
  journal={arXiv preprint arXiv:2308.09081},
  year={2023}
}

@book{MacKay2003,
  added-at = {2007-05-24T14:43:04.000+0200},
  author = {MacKay, David J. C.},
  biburl = {https://www.bibsonomy.org/bibtex/24c23fea472f6e75c0964badd83883d77/tmalsburg},
  interhash = {86f621d9d6f9f159448f768d792d4511},
  intrahash = {4c23fea472f6e75c0964badd83883d77},
  keywords = {bayesianinference book informationtheory neuralnetworks patternrecognition probabilitytheory},
  publisher = {Copyright Cambridge University Press},
  timestamp = {2007-05-24T14:43:04.000+0200},
  title = {Information Theory, Inference, and Learning Algorithms},
  doi = {10.1109/tit.2004.834752},
  year = 2003
}

@article{Rodriguez2021EntropyEst,
  title={Selecting an effective entropy estimator for short sequences of bits and bytes with maximum entropy},
  author={Contreras Rodr{\'\i}guez, Lianet and Madarro-Cap{\'o}, Evaristo Jos{\'e} and Leg{\'o}n-P{\'e}rez, Carlos Miguel and Rojas, Omar and Sosa-G{\'o}mez, Guillermo},
  journal={Entropy},
  volume={23},
  number={5},
  pages={561},
  year={2021},
  doi={10.3390/e23050561},
  publisher={MDPI}
}

@misc{grassberger2008entropy,
      title={Entropy Estimates from Insufficient Samplings},
      author={P. Grassberger},
      year={2008},
      eprint={physics/0307138},
      archivePrefix={arXiv},
      primaryClass={physics.data-an}
}

@article{chaoshen,
author = {Chao, Anne and Shen, Tsung-Jen},
year = {2003},
month = {12},
pages = {429-443},
title = {Chao A, Shen T-J.. Nonparametric estimation of Shannon's diversity index when there are unseen species in sample. Environ Ecol Stat 10: 429-443},
volume = {10},
journal = {Environmental and Ecological Statistics},
doi = {10.1023/A:1026096204727}
}


@inproceedings{10.5555/2980539.2980601,
  author = {Nemenman, Ilya and Shafee, Fariel and Bialek, William},
  title = {Entropy and inference, revisited},
  year = {2001},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
  pages = {471â€“478},
  numpages = {8},
  location = {Vancouver, British Columbia, Canada},
  series = {NIPS'01},
  doi={10.7551/mitpress/1120.003.0065},
}

@article{zhang,
    author = {Zhang, Zhiyi},
    title = "{Entropy Estimation in Turing's Perspective}",
    journal = {Neural Computation},
    volume = {24},
    number = {5},
    pages = {1368-1389},
    year = {2012},
    month = {05},
    abstract = "{A new nonparametric estimator of Shannon's entropy on a countable alphabet is proposed and analyzed against the well-known plug-in estimator. The proposed estimator is developed based on Turing's formula, which recovers distributional characteristics on the subset of the alphabet not covered by a size-n sample. The fundamental switch in perspective brings about substantial gain in estimation accuracy for every distribution with finite entropy. In general, a uniform variance upper bound is established for the entire class of distributions with finite entropy that decays at a rate of O(ln(n)/n) compared to O([ln(n)]2/n) for the plug-in. In a wide range of subclasses, the variance of the proposed estimator converges at a rate of O(1/n), and this rate of convergence carries over to the convergence rates in mean squared errors in many subclasses. Specifically, for any finite alphabet, the proposed estimator has a bias decaying exponentially in n. Several new bias-adjusted estimators are also discussed.}",
    issn = {0899-7667},
    doi = {10.1162/NECO_a_00266},
    url = {https://doi.org/10.1162/NECO\_a\_00266},
    eprint = {https://direct.mit.edu/neco/article-pdf/24/5/1368/865674/neco\_a\_00266.pdf},
}

@misc{hausser2009entropy,
      title={Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks},
      author={Jean Hausser and Korbinian Strimmer},
      year={2009},
      eprint={0811.3579},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

