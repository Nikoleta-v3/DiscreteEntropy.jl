var documenterSearchIndex = {"docs":
[{"location":"#DiscreteEntropy","page":"Home","title":"DiscreteEntropy","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for the estimation of Shannon entropy of discrete distributions.","category":"page"},{"location":"#Multiplicities","page":"Home","title":"Multiplicities","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DiscreteEntropy uses the multiplicities representation of data. Given a histogram of samples","category":"page"},{"location":"#Types","page":"Home","title":"Types","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"AbstractEstimator\nestimate_h","category":"page"},{"location":"#DiscreteEntropy.estimate_h","page":"Home","title":"DiscreteEntropy.estimate_h","text":"estimate_h(data::CountData, ::Type{T}) where {T<:AbstractEstimator}\n\n\n\n\n\n","category":"function"},{"location":"#Data","page":"Home","title":"Data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"from_data\nfrom_samples\nfrom_counts\n\nEntropyData\nCountData","category":"page"},{"location":"#DiscreteEntropy.from_data","page":"Home","title":"DiscreteEntropy.from_data","text":"from_data(data::AbstractVector, ::Type{Samples})\nfrom_data(data::AbstractVector, ::Type{SampleHistogram})\n\n\n\n\n\n","category":"function"},{"location":"#DiscreteEntropy.from_samples","page":"Home","title":"DiscreteEntropy.from_samples","text":"from_samples\n\n\n\n\n\n","category":"function"},{"location":"#DiscreteEntropy.CountData","page":"Home","title":"DiscreteEntropy.CountData","text":"CountData\n\n\n\n\n\n","category":"type"},{"location":"#Frequentist-Estimators","page":"Home","title":"Frequentist Estimators","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"maximum_likelihood\njackknife_ml\nmiller_madow\ngrassberger\nschurmann\nschurmann_generalised\nzhang\nchao_shen\nbonachela","category":"page"},{"location":"#DiscreteEntropy.maximum_likelihood","page":"Home","title":"DiscreteEntropy.maximum_likelihood","text":"maximum_likelihood(data::CountData)::Float64\n\nReturns the maximum likelihood estimation of Shannon entropy.\n\nhatH_ML = log(n) - frac1n sum_k=1^Kh_k log(h_k)\n\nwhere n is the number of samples\n\n\n\n\n\n","category":"function"},{"location":"#Bayesian-Estimators","page":"Home","title":"Bayesian Estimators","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"bayes\nnsb\nansb\npym","category":"page"},{"location":"#Resampling","page":"Home","title":"Resampling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We can also resample data","category":"page"},{"location":"","page":"Home","title":"Home","text":"jackknife","category":"page"},{"location":"#Divergence","page":"Home","title":"Divergence","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"kl_divergence\njeffreys_divergence\njensen_shannon_divergence\njensen_shannon_distance","category":"page"},{"location":"#Conditional-Entropy-and-Conditional-Mutual-Information","page":"Home","title":"Conditional Entropy and Conditional Mutual Information","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Conditional Entropy","category":"page"},{"location":"","page":"Home","title":"Home","text":"conditional_entropy","category":"page"},{"location":"#Utilities","page":"Home","title":"Utilities","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"logx\nxlogx\nto_bits\nto_bans","category":"page"}]
}
