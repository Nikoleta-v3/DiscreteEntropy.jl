<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · DiscreteEntropy.jl</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>DiscreteEntropy.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Multiplicities"><span>Multiplicities</span></a></li><li><a class="tocitem" href="#Types"><span>Types</span></a></li><li><a class="tocitem" href="#Data"><span>Data</span></a></li><li><a class="tocitem" href="#Frequentist-Estimators"><span>Frequentist Estimators</span></a></li><li><a class="tocitem" href="#Bayesian-Estimators"><span>Bayesian Estimators</span></a></li><li><a class="tocitem" href="#Resampling"><span>Resampling</span></a></li><li><a class="tocitem" href="#Divergence"><span>Divergence</span></a></li><li><a class="tocitem" href="#Conditional-Entropy-and-Conditional-Mutual-Information"><span>Conditional Entropy and Conditional Mutual Information</span></a></li><li><a class="tocitem" href="#Other-Useful-Measures"><span>Other Useful Measures</span></a></li><li><a class="tocitem" href="#Utilities"><span>Utilities</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kellino/DiscreteEntropy.jl/blob/main/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="DiscreteEntropy"><a class="docs-heading-anchor" href="#DiscreteEntropy">DiscreteEntropy</a><a id="DiscreteEntropy-1"></a><a class="docs-heading-anchor-permalink" href="#DiscreteEntropy" title="Permalink"></a></h1><p>A <a href="http://julialang.org">Julia</a> package for the estimation of Shannon entropy of discrete distributions.</p><h2 id="Multiplicities"><a class="docs-heading-anchor" href="#Multiplicities">Multiplicities</a><a id="Multiplicities-1"></a><a class="docs-heading-anchor-permalink" href="#Multiplicities" title="Permalink"></a></h2><p>DiscreteEntropy uses the multiplicities representation of data. Given a histogram of samples</p><h2 id="Types"><a class="docs-heading-anchor" href="#Types">Types</a><a id="Types-1"></a><a class="docs-heading-anchor-permalink" href="#Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.AbstractEstimator" href="#DiscreteEntropy.AbstractEstimator"><code>DiscreteEntropy.AbstractEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractEstimator</code></pre><p>Supertype for NonParameterised and Parameterised entropy estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/estimate.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.estimate_h" href="#DiscreteEntropy.estimate_h"><code>DiscreteEntropy.estimate_h</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">estimate_h(data::CountData, estimator::Type{T}) where {T&lt;:AbstractEstimator}</code></pre><p>Return the estimate in nats of Shannon entropy of <code>data</code> using <code>estimator</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/estimate.jl#L40-L45">source</a></section></article><h2 id="Data"><a class="docs-heading-anchor" href="#Data">Data</a><a id="Data-1"></a><a class="docs-heading-anchor-permalink" href="#Data" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.from_data" href="#DiscreteEntropy.from_data"><code>DiscreteEntropy.from_data</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">from_data(data::AbstractVector, ::Type{Samples})
from_data(data::AbstractVector, ::Type{SampleHistogram})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Core/countdata.jl#L62-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.from_samples" href="#DiscreteEntropy.from_samples"><code>DiscreteEntropy.from_samples</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">from_samples</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Core/countdata.jl#L74-L76">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>from_counts</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>EntropyData</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.CountData" href="#DiscreteEntropy.CountData"><code>DiscreteEntropy.CountData</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CountData</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Core/countdata.jl#L13-L15">source</a></section></article><h2 id="Frequentist-Estimators"><a class="docs-heading-anchor" href="#Frequentist-Estimators">Frequentist Estimators</a><a id="Frequentist-Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Frequentist-Estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.maximum_likelihood" href="#DiscreteEntropy.maximum_likelihood"><code>DiscreteEntropy.maximum_likelihood</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">maximum_likelihood(data::CountData)::Float64</code></pre><p>Compute the maximum likelihood estimation of Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{ML} = \log(N) - \frac{1}{N} \sum_{i=1}^{K}h_i \log(h_i)\]</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">
julia&gt; data = from_data([1,2,3,2,1], Histogram)
CountData([2.0 3.0 1.0; 2.0 1.0 2.0], 9.0, 6)

julia&gt; maximum_likelihood(data)
1.522955067</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Frequentist/frequentist.jl#L8-L26">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>jackknife_ml</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.miller_madow" href="#DiscreteEntropy.miller_madow"><code>DiscreteEntropy.miller_madow</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">miller_madow(data::CountData)</code></pre><p>Returns the maximum likelihood estimation of Shannon entropy, with a positive offset based on the total number of samples seen (N) and the support size (K).</p><p class="math-container">\[\hat{H}_{MM} = \hat{H}_{ML} + \frac{K - 1}{2N}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Frequentist/frequentist.jl#L58-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.grassberger" href="#DiscreteEntropy.grassberger"><code>DiscreteEntropy.grassberger</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">grassberger(data::CountData)</code></pre><p>Returns the Grassberger estimation of Shannon entropy.</p><p class="math-container">\[\hat{H}_G = log(N) - \frac{1}{N} \sum_{i=1}^{K} h_i \; G(h_i)\]</p><p>This is essentially the same as <span>$\hat{H}_{ML}$</span>, but with the logarithm swapped for the scalar function <span>$G$</span></p><p>where</p><p class="math-container">\[G(h) = \psi(h) + \frac{1}{2}(-1)^h \big( \psi(\frac{h+1}{2} - \psi(\frac{h}{2}))\]</p><p>This is the solution to <span>$G(h) = \psi(h) + (-1)^h \int_0^1 \frac{x^h - 1}{x+1} dx$</span> as given in the <a href="https://arxiv.org/pdf/physics/0307138v2.pdf">paper</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Frequentist/frequentist.jl#L75-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.schurmann" href="#DiscreteEntropy.schurmann"><code>DiscreteEntropy.schurmann</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">schurmann(data::CountData, ξ::Float64 = ℯ^(-1/2))</code></pre><p><a href="https://arxiv.org/pdf/cond-mat/0403192.pdf">schurmann</a></p><p class="math-container">\[\hat{H}_{SHU} = \psi(N) - \frac{1}{N} \sum_{i=1}^{K} \, h_i \big( \psi(h_i) + (-1)^{h_i} ∫_0^{\frac{1}{\xi} - 1} \frac{t^{h_i}-1}{1+t}dt \big)
\]</p><p>This is no one ideal value for <span>$\xi$</span>, however the paper suggests <span>$e^{(-1/2)} \approx 0.6$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Frequentist/frequentist.jl#L109-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.schurmann_generalised" href="#DiscreteEntropy.schurmann_generalised"><code>DiscreteEntropy.schurmann_generalised</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">schurmann_generalised(data::CountVector, xis::XiVector{T}) where {T&lt;:Real}</code></pre><p><a href="https://arxiv.org/pdf/2111.11175.pdf">schurmann_generalised</a></p><p class="math-container">\[\hat{H}_{SHU} = \psi(N) - \frac{1}{N} \sum_{i=1}^{K} \, h_i \big( \psi(h_i) + (-1)^{h_i} ∫_0^{\frac{1}{\xi_i} - 1} \frac{t^{h_i}-1}{1+t}dt \big)
\]</p><p>Computes the generalised Schurmann entropy estimation, given a countvector <em>data</em> and a xivector <em>xis</em>, which must both be the same length.</p><pre><code class="nohighlight hljs">schurmann_generalised(data::CountVector, xis::Distribution, scalar=false)</code></pre><p>Computes the generalised Schurmann entropy estimation, given a countvector <em>data</em> and a distribution <em>xis</em>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Frequentist/frequentist.jl#L134-L151">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.zhang" href="#DiscreteEntropy.zhang"><code>DiscreteEntropy.zhang</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">zhang(data::CountData)</code></pre><p>Return the Zhang estimate of the Shannon entropy of <code>data</code> in nats.</p><p>The recommended definition of Zhang&#39;s estimator is from <a href="https://www.tandfonline.com/doi/full/10.1080/09296174.2013.830551">Grabchak <em>et al.</em></a></p><p class="math-container">\[\hat{H}_Z = \sum_{i=1}^K \hat{p}_i \sum_{v=1}^{N - h_i} \frac{1}{v} ∏_{j=0}^{v-1} \left( 1 + \frac{1 - h_i}{N - 1 - j} \right)\]</p><p>The actual algorithm comes from <a href="https://arxiv.org/abs/1707.08290">Fast Calculation of entropy with Zhang&#39;s estimator</a> by Lozano <em>et al.</em>.</p><p><strong>Links</strong></p><p><a href="https://dl.acm.org/doi/10.1162/NECO_a_00266">Entropy estimation in turing&#39;s perspective</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Frequentist/frequentist.jl#L208-L222">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.chao_shen" href="#DiscreteEntropy.chao_shen"><code>DiscreteEntropy.chao_shen</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">chao_shen(data::CountData)</code></pre><p>Return the Chao-Shen estimate of the Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{CS} = - \sum_{i=i}^{K} \frac{\hat{p}_i^{CS} \log \hat{p}_i^{CS}}{1 - (1 - \hat{p}_i^{CS})}\]</p><p>where</p><p class="math-container">\[\hat{p}_i^{CS} = (1 - \frac{1 - \hat{p}_i^{ML}}{N}) \hat{p}_i^{ML}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Frequentist/frequentist.jl#L175-L188">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>bonachela</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Bayesian-Estimators"><a class="docs-heading-anchor" href="#Bayesian-Estimators">Bayesian Estimators</a><a id="Bayesian-Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.bayes" href="#DiscreteEntropy.bayes"><code>DiscreteEntropy.bayes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bayes(data::CountData, α::AbstractFloat)</code></pre><p>Returns an estimate of Shannon entropy given data and a concentration parameter <span>$α$</span>.</p><p class="math-container">\[\hat{H}_{\text{Bayes}} = - \sum_{k=1}^{K} \hat{p}_k^{\text{Bayes}} \; \log \hat{p}_k^{\text{Bayes}}\]</p><p>where</p><p class="math-container">\[p_k^{\text{Bayes}} = \frac{k + α}{n + A}\]</p><p>and</p><p class="math-container">\[A = \sum_{x=1}^{K} α_{x}\]</p><p>In addition to setting your own α, we have the following suggested choices</p><ol><li><a href="https://ieeexplore.ieee.org/document/1056331">jeffrey</a> : α = 0.5</li><li>laplace: α = 1.0</li><li>schurmann_grassberger: α = 1 / K</li><li>minimax: α = √{n} / K</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Bayesian/bayesian.jl#L4-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.nsb" href="#DiscreteEntropy.nsb"><code>DiscreteEntropy.nsb</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">nsb(data, K=data.K)</code></pre><p>Returns the Bayesian estimate of Shannon entropy of data, using the Nemenman, Shafee, Bialek algorithm</p><p class="math-container">\[\hat{H}^{\text{NSB}} = \frac{ \int_0^{\ln(K)} d\xi \, \rho(\xi, \textbf{n}) \langle H^m \rangle_{\beta (\xi)}  }
                            { \int_0^{\ln(K)} d\xi \, \rho(\xi\mid n)}\]</p><p>where</p><p class="math-container">\[\rho(\xi \mid \textbf{n}) =
    \mathcal{P}(\beta (\xi)) \frac{ \Gamma(\kappa(\xi))}{\Gamma(N + \kappa(\xi))}
    \prod_{i=1}^K \frac{\Gamma(n_i + \beta(\xi))}{\Gamma(\beta(\xi))}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Bayesian/nsb.jl#L94-L111">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.ansb" href="#DiscreteEntropy.ansb"><code>DiscreteEntropy.ansb</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ansb(data::CountData; undersampled::Float64=0.1)::Float64</code></pre><p class="math-container">\[\hat{H}_{ANSB} = \frac{C_\gamma}{\ln(2)} - 1 + 2 \ln(N) - \psi_0(\Delta)\]</p><p>where <span>$C_\gamma$</span> is Euler&#39;s Gamma Constant <span>$\approx 0.57721...$</span>, <span>$\psi_0$</span> is the digamma function and <span>$\Delta$</span> the number of coincidences in the data.</p><p>Returns the <a href="https://arxiv.org/pdf/physics/0306063.pdf">Asymptotic NSB estimator</a> (equations 11 and 12)</p><p>This is designed for the extremely undersampled regime (K ~ N) and diverges with N when well-sampled. ANSB requires that <span>$N/K → 0$</span>, which we set to be <span>$N/K &lt; 0.1$</span> by default</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Estimators/Bayesian/nsb.jl#L8-L22">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>pym</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Resampling"><a class="docs-heading-anchor" href="#Resampling">Resampling</a><a id="Resampling-1"></a><a class="docs-heading-anchor-permalink" href="#Resampling" title="Permalink"></a></h2><p>We can also resample data</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>jackknife</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Divergence"><a class="docs-heading-anchor" href="#Divergence">Divergence</a><a id="Divergence-1"></a><a class="docs-heading-anchor-permalink" href="#Divergence" title="Permalink"></a></h2><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>kl_divergence</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>jeffreys_divergence</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>jensen_shannon_divergence</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>jensen_shannon_distance</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Conditional-Entropy-and-Conditional-Mutual-Information"><a class="docs-heading-anchor" href="#Conditional-Entropy-and-Conditional-Mutual-Information">Conditional Entropy and Conditional Mutual Information</a><a id="Conditional-Entropy-and-Conditional-Mutual-Information-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-Entropy-and-Conditional-Mutual-Information" title="Permalink"></a></h2><p><a href="https://en.wikipedia.org/wiki/Conditional_entropy">Conditional Entropy</a></p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>conditional_entropy</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Other-Useful-Measures"><a class="docs-heading-anchor" href="#Other-Useful-Measures">Other Useful Measures</a><a id="Other-Useful-Measures-1"></a><a class="docs-heading-anchor-permalink" href="#Other-Useful-Measures" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.redundancy" href="#DiscreteEntropy.redundancy"><code>DiscreteEntropy.redundancy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">redundancy(data::CountData, estimator::Type{T}) where {T&lt;:AbstractEstimator}</code></pre><p>Return the estimated information redundancy of <code>data</code>, with <code>K</code> the sampled support size.</p><p class="math-container">\[R = \log(K) - \hat{H}(data)\]</p><pre><code class="nohighlight hljs">redundancy(data::CountData, estimator::Type{T}, K::Int64) where {T&lt;:AbstractEstimator}</code></pre><p>Return the estimated information redundancy of <code>data</code>, with <code>K</code> set by the user.</p><p><strong>External Links</strong></p><p>(https://en.wikipedia.org/wiki/Redundancy<em>(information</em>theory))</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/InfoTheory/mutual_information.jl#L12-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.uncertainty_coefficient" href="#DiscreteEntropy.uncertainty_coefficient"><code>DiscreteEntropy.uncertainty_coefficient</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">uncertainty_coefficient(counts::Matrix)</code></pre><p class="math-container">\[C_{XY} = \frac{I(X;Y)}{H(Y)}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/InfoTheory/mutual_information.jl#L1-L7">source</a></section></article><h2 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.logx" href="#DiscreteEntropy.logx"><code>DiscreteEntropy.logx</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">logx(x)::Float64</code></pre><p>Returns natural logarithm of x, or 0.0 if x is zero</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Core/utils.jl#L3-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.xlogx" href="#DiscreteEntropy.xlogx"><code>DiscreteEntropy.xlogx</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">xlogx(x::Float64)</code></pre><p>Returns <code>x * log(x)</code> for <code>x ≥ 0</code>, or 0.0 if x is zero</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Core/utils.jl#L14-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.to_bits" href="#DiscreteEntropy.to_bits"><code>DiscreteEntropy.to_bits</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">to_bits(x::Float64)</code></pre><p>Return <span>$\frac{h}{\log(2)}$</span> where h is in nats</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Core/utils.jl#L30-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiscreteEntropy.to_bans" href="#DiscreteEntropy.to_bans"><code>DiscreteEntropy.to_bans</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">to_bans(x::Float64)</code></pre><p>Return <span>$\frac{h}{log(10)}$</span> where <code>h</code> is in nats</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/3bcd9e64cee700b746ad08ef442330174f9c9b9c/src/Core/utils.jl#L38-L41">source</a></section></article></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Thursday 9 March 2023 16:23">Thursday 9 March 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
