<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · DiscreteEntropy.jl</title><meta name="title" content="Home · DiscreteEntropy.jl"/><meta property="og:title" content="Home · DiscreteEntropy.jl"/><meta property="twitter:title" content="Home · DiscreteEntropy.jl"/><meta name="description" content="Documentation for DiscreteEntropy.jl."/><meta property="og:description" content="Documentation for DiscreteEntropy.jl."/><meta property="twitter:description" content="Documentation for DiscreteEntropy.jl."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>DiscreteEntropy.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Multiplicities"><span>Multiplicities</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/kellino/DiscreteEntropy.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/kellino/DiscreteEntropy.jl/blob/master/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="DiscreteEntropy"><a class="docs-heading-anchor" href="#DiscreteEntropy">DiscreteEntropy</a><a id="DiscreteEntropy-1"></a><a class="docs-heading-anchor-permalink" href="#DiscreteEntropy" title="Permalink"></a></h1><p>A <a href="http://julialang.org">Julia</a> package for the estimation of Shannon entropy of discrete distributions.</p><h2 id="Multiplicities"><a class="docs-heading-anchor" href="#Multiplicities">Multiplicities</a><a id="Multiplicities-1"></a><a class="docs-heading-anchor-permalink" href="#Multiplicities" title="Permalink"></a></h2><p>DiscreteEntropy uses the multiplicities representation of data. Given a histogram of samples</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.AbstractEstimator" href="#DiscreteEntropy.AbstractEstimator"><code>DiscreteEntropy.AbstractEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractEstimator</code></pre><p>Supertype for NonParameterised and Parameterised entropy estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/estimate.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.CountData" href="#DiscreteEntropy.CountData"><code>DiscreteEntropy.CountData</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CountData
an 2 x m matrix where m[1, :] is counts and m[2, :] the number of bins with those counts
[[2 3 1] =&gt; counts / icts
[2 1 2]] =&gt; bins / mm
so we have two bins with two, 1 bin with 3, and 2 bins with 1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/countdata.jl#L13-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.EntropyData" href="#DiscreteEntropy.EntropyData"><code>DiscreteEntropy.EntropyData</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type EntropyData</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/countdata.jl#L6-L8">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.ansb-Tuple{CountData}" href="#DiscreteEntropy.ansb-Tuple{CountData}"><code>DiscreteEntropy.ansb</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ansb(data::CountData; undersampled::Float64=0.1)::Float64</code></pre><p>Return the Asymptotic NSB estimation of the Shannon entropy of <code>data</code> in nats.</p><p>See <a href="https://arxiv.org/pdf/physics/0306063.pdf">Asymptotic NSB estimator</a> (equations 11 and 12)</p><p class="math-container">\[\hat{H}_{\tiny{ANSB}} = (C_\gamma - \log(2)) + 2 \log(N) - \psi(\Delta)\]</p><p>where <span>$C_\gamma$</span> is Euler&#39;s Gamma (<span>$\approx 0.57721...$</span>), <span>$\psi_0$</span> is the digamma function and <span>$\Delta$</span> the number of coincidences in the data.</p><p>This is designed for the extremely undersampled regime (K ~ N) and diverges with N when well-sampled. ANSB requires that <span>$N/K → 0$</span>, which we set to be <span>$N/K &lt; 0.1$</span> by default</p><p><strong>External Links</strong></p><p><a href="https://arxiv.org/pdf/physics/0306063.pdf">Asymptotic NSB estimator</a> (equations 11 and 12)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Bayesian/nsb.jl#L8-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.bonachela-Tuple{CountData}" href="#DiscreteEntropy.bonachela-Tuple{CountData}"><code>DiscreteEntropy.bonachela</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">bonachela(data::CountData)</code></pre><p>Return the Bonachela estimator of the Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{B} = \frac{1}{N+2} \sum_{i=1}^{K} \left( (h_i + 1) \sum_{j=n_i + 2}^{N+2} \frac{1}{j} \right)\]</p><p><strong>External Links</strong></p><p><a href="https://arxiv.org/pdf/0804.4561.pdf">Entropy estimates of small data sets</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L227-L238">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.chao_shen-Tuple{CountData}" href="#DiscreteEntropy.chao_shen-Tuple{CountData}"><code>DiscreteEntropy.chao_shen</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">chao_shen(data::CountData)</code></pre><p>Return the Chao-Shen estimate of the Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{CS} = - \sum_{i=i}^{K} \frac{\hat{p}_i^{CS} \log \hat{p}_i^{CS}}{1 - (1 - \hat{p}_i^{CS})}\]</p><p>where</p><p class="math-container">\[\hat{p}_i^{CS} = (1 - \frac{1 - \hat{p}_i^{ML}}{N}) \hat{p}_i^{ML}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L159-L172">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.chao_wang_jost-Tuple{CountData}" href="#DiscreteEntropy.chao_wang_jost-Tuple{CountData}"><code>DiscreteEntropy.chao_wang_jost</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">chao_wang_jost(data::CountData)</code></pre><p>Return the Chao Wang Jost Shannon entropy estimate of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{\tiny{CWJ}} = \sum_{1 \leq h_i \leq N-1} \frac{h_i}{N} \left(\sum_{k=h_i}^{N-1} \frac{1}{k} \right) +
\frac{f_1}{N} (1 - A)^{-N + 1} \left\{ - \log(A) - \sum_{r=1}^{N-1} \frac{1}{r} (1 - A)^r \right\}\]</p><p>with</p><p class="math-container">\[A = \begin{cases}
\frac{2 f_2}{(N-1) f_1 + 2 f_2} \, &amp; \text{if} \, f_2 &gt; 0 \\
\frac{2}{(N-1)(f_1 - 1) + 1} \, &amp; \text{if} \, f_2 = 0, \; f_1 \neq 0 \\
1, &amp; \text{if} \, f_1 = f_2 = 0
\end{cases}\]</p><p>where <span>$f_1$</span> is the number of singletons and <span>$f_2$</span> the number of doubletons in <code>data</code>.</p><p><strong>Notes</strong></p><p>The algorithm is slightly modified port of that used in the <a href="https://github.com/EricMarcon/entropart/blob/master/R/Shannon.R">entropart</a> R library.</p><p><strong>External Links</strong></p><p><a href="https://chao.stat.nthu.edu.tw/wordpress/paper/99.pdf">Entropy and the species accumulation curve: a novel entropy estimator via discovery rates of new species</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L326-L354">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.conditional_entropy-Tuple{CountVector, CountVector}" href="#DiscreteEntropy.conditional_entropy-Tuple{CountVector, CountVector}"><code>DiscreteEntropy.conditional_entropy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">conditional_entropy(pmfX::AbstractVector{AbstractFloat}, pmfXY::AbstractVector{AbstractFloat})
conditional_entropy(X::CountData, XY::CountData, estimator::Type{T}) where {T&lt;:NonParameterisedEstimator}
conditional_entropy(X::CountData, XY::CountData, estimator::Type{T}, param) where {T&lt;:ParameterisedEstimator}</code></pre><p>Compute the conditional entropy of Y conditioned on X</p><p class="math-container">\[H(Y \mid X) = - \sum_{x \in X, y \in Y} p(x, y) \ln \frac{p(x, y)}{p(x)}\]</p><p>Compute the estimated conditional entropy of Y given X, from counts of X, and (X,Y) and estimator <em>estimator</em></p><p class="math-container">\[\hat{H}(Y \mid X) = \hat{H}(X, Y) - \hat{H}(X)\]</p><pre><code class="nohighlight hljs"></code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/InfoTheory/conditional_entropy.jl#L1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.from_counts-Tuple{CountVector, Bool}" href="#DiscreteEntropy.from_counts-Tuple{CountVector, Bool}"><code>DiscreteEntropy.from_counts</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs"> from_counts(counts::CountVector, remove_zeros::Bool)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/countdata.jl#L74-L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.from_data-Tuple{AbstractVector, Type{Samples}}" href="#DiscreteEntropy.from_data-Tuple{AbstractVector, Type{Samples}}"><code>DiscreteEntropy.from_data</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">from_data(data::AbstractVector, ::Type{Samples})
from_data(data::AbstractVector, ::Type{Histogram})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/countdata.jl#L108-L111">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.from_samples-Tuple{String, Any}" href="#DiscreteEntropy.from_samples-Tuple{String, Any}"><code>DiscreteEntropy.from_samples</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">from_samples</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/countdata.jl#L124-L126">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.grassberger-Tuple{CountData}" href="#DiscreteEntropy.grassberger-Tuple{CountData}"><code>DiscreteEntropy.grassberger</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">grassberger(data::CountData)</code></pre><p>Return the Grassberger (1988) estimation of Shannon entropy of <code>data</code> in nats</p><p class="math-container">\[\hat{H}_{\tiny{Gr88}} = \sum_i \frac{h_i}{H} \left(\log(N) - \psi(h_i) - \frac{(-1)^{h_i}}{n_i + 1}  \right)\]</p><p>Equation 13 from <a href="https://www.academia.edu/download/47091312/0375-9601_2888_2990193-420160707-16069-1k3ppo7.pdf">Finite sample corrections to entropy and dimension estimate</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L63-L73">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.jackknife-Tuple{CountData, Function}" href="#DiscreteEntropy.jackknife-Tuple{CountData, Function}"><code>DiscreteEntropy.jackknife</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">jackknife(data::CountData, statistic::Function; corrected=false)</code></pre><p>Compute the jackknifed estimate of <em>statistic</em> on data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/resample.jl#L66-L70">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.jackknife_mle-Tuple{CountData}" href="#DiscreteEntropy.jackknife_mle-Tuple{CountData}"><code>DiscreteEntropy.jackknife_mle</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">jackknife_mle(data::CountData; corrected=false)::Tuple{AbstractFloat, AbstractFloat}</code></pre><p>Return the <em>jackknifed</em> estimate of data and the variance of the jackknifing (not the variance of the estimator itself).</p><p>If corrected is true, then the variance is scaled with n-1, else it is scaled with n</p><p>As found in the <a href="https://academic.oup.com/biomet/article/65/3/625/234287">paper</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L30-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.jeffreys_divergence-Tuple{Any, Any}" href="#DiscreteEntropy.jeffreys_divergence-Tuple{Any, Any}"><code>DiscreteEntropy.jeffreys_divergence</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">jeffreys_divergence(p, q)
(link)[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7516653/]</code></pre><p class="math-container">\[J(p, q) = D_{KL}(p \Vert q) + D_{KL}(q \Vert p)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/InfoTheory/divergence.jl#L62-L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.jensen_shannon_distance-Tuple{AbstractVector, AbstractVector, Function}" href="#DiscreteEntropy.jensen_shannon_distance-Tuple{AbstractVector, AbstractVector, Function}"><code>DiscreteEntropy.jensen_shannon_distance</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">jensen_shannon_distance(P::AbstractVector, Q::AbstractVector, estimator)</code></pre><p>Compute the Jensen Shannon Distance</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/InfoTheory/divergence.jl#L52-L57">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.jensen_shannon_divergence-Tuple{AbstractVector, AbstractVector}" href="#DiscreteEntropy.jensen_shannon_divergence-Tuple{AbstractVector, AbstractVector}"><code>DiscreteEntropy.jensen_shannon_divergence</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">jenson_shannon_divergence(countsP::AbstractVector, countsQ::AbstractVector)
jenson_shannon_divergence(countsP::AbstractVector, countsQ::AbstractVector, estimator::Type{T}) where {T&lt;:NonParamterisedEstimator}</code></pre><p>Compute the Jenson Shannon Divergence between discrete distributions <span>$P$</span> and <span>$q$</span>, as represented by their histograms.</p><p class="math-container">\[\widehat{JS}(p, q) = \hat{H}\left(\frac{p + q}{2} \right) - \left( \frac{H(p) + H(q)}{2} \right)
\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/InfoTheory/divergence.jl#L24-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.kl_divergence-Tuple{CountVector, CountVector}" href="#DiscreteEntropy.kl_divergence-Tuple{CountVector, CountVector}"><code>DiscreteEntropy.kl_divergence</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kl_divergence(p::AbstractVector, q::AbstractVector)::Float64</code></pre><p class="math-container">\[D_{KL}(P ‖ Q) = \sum_{x \in X} P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]</p><p>Compute the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Interpretations">Kullback-Lebler Divergence</a> between two discrete distributions. Both distributions needs to be defined over the same space, so length(p) == length(q). If the distributions are not normalised, they will be.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/InfoTheory/divergence.jl#L3-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.logx-Tuple{Any}" href="#DiscreteEntropy.logx-Tuple{Any}"><code>DiscreteEntropy.logx</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">logx(x)::Float64</code></pre><p>Returns natural logarithm of x, or 0.0 if x is zero</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/utils.jl#L6-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.marginal_counts-Tuple{Matrix, Any}" href="#DiscreteEntropy.marginal_counts-Tuple{Matrix, Any}"><code>DiscreteEntropy.marginal_counts</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">marginal_counts(contingency_matrix::Matrix, dim)</code></pre><p>Return the <em>unnormalised</em> marginal counts of <code>contingency_matrix</code> along dimension <code>dim</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/utils.jl#L64-L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.maximum_likelihood-Tuple{CountData}" href="#DiscreteEntropy.maximum_likelihood-Tuple{CountData}"><code>DiscreteEntropy.maximum_likelihood</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">maximum_likelihood(data::CountData)::Float64</code></pre><p>Return the maximum likelihood estimation of Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{\tiny{ML}} = - \sum_{i=1}^K p_i \log(p_i)\]</p><p>or equivalently</p><p class="math-container">\[\hat{H}_{\tiny{ML}} = \log(N) - \frac{1}{N} \sum_{i=1}^{K}h_i \log(h_i)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L7-L20">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.miller_madow-Tuple{CountData}" href="#DiscreteEntropy.miller_madow-Tuple{CountData}"><code>DiscreteEntropy.miller_madow</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">miller_madow(data::CountData)</code></pre><p>Return the Miller Madow estimation of Shannon entropy, with a positive bias based on the total number of samples seen (N) and the support size (K).</p><p class="math-container">\[\hat{H}_{\tiny{MM}} = \hat{H}_{\tiny{ML}} + \frac{K - 1}{2N}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L46-L55">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.nsb-Tuple{CountData, Any}" href="#DiscreteEntropy.nsb-Tuple{CountData, Any}"><code>DiscreteEntropy.nsb</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">nsb(data, K=data.K)</code></pre><p>Returns the Bayesian estimate of Shannon entropy of data, using the Nemenman, Shafee, Bialek algorithm</p><p class="math-container">\[\hat{H}^{\text{NSB}} = \frac{ \int_0^{\ln(K)} d\xi \, \rho(\xi, \textbf{n}) \langle H^m \rangle_{\beta (\xi)}  }
                            { \int_0^{\ln(K)} d\xi \, \rho(\xi\mid n)}\]</p><p>where</p><p class="math-container">\[\rho(\xi \mid \textbf{n}) =
    \mathcal{P}(\beta (\xi)) \frac{ \Gamma(\kappa(\xi))}{\Gamma(N + \kappa(\xi))}
    \prod_{i=1}^K \frac{\Gamma(n_i + \beta(\xi))}{\Gamma(\beta(\xi))}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Bayesian/nsb.jl#L114-L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.pert-Union{Tuple{T}, Tuple{CountData, Type{T}}} where T&lt;:AbstractEstimator" href="#DiscreteEntropy.pert-Union{Tuple{T}, Tuple{CountData, Type{T}}} where T&lt;:AbstractEstimator"><code>DiscreteEntropy.pert</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pert(data::CountData, estimator)</code></pre><p>A Pert estimate of entropy, where</p><pre><code class="nohighlight hljs">H = \frac{a + 4b + c}{6}</code></pre><p>where a is the minimum (maximum_likelihood), c is the maximum (log(k)) and <span>$b$</span> is the most likely value (ChaoShen)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/estimate.jl#L167-L177">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.pym-Tuple{Vector{Int64}, Vector{Int64}}" href="#DiscreteEntropy.pym-Tuple{Vector{Int64}, Vector{Int64}}"><code>DiscreteEntropy.pym</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs"> pym(_mm::Vector{Int64}, _icts::Vector{Int64})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Bayesian/pym.jl#L345-L348">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.schurmann" href="#DiscreteEntropy.schurmann"><code>DiscreteEntropy.schurmann</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">schurmann(data::CountData, ξ::Float64 = ℯ^(-1/2))</code></pre><p>Return the Schurmann estimate of Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{SHU} = \psi(N) - \frac{1}{N} \sum_{i=1}^{K} \, h_i \left( \psi(h_i) + (-1)^{h_i} ∫_0^{\frac{1}{\xi} - 1} \frac{t^{h_i}-1}{1+t}dt \right)
\]</p><p>This is no one ideal value for <span>$\xi$</span>, however the paper suggests <span>$e^{(-1/2)} \approx 0.6$</span></p><p><strong>External Links</strong></p><p><a href="https://arxiv.org/pdf/cond-mat/0403192.pdf">schurmann</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L86-L99">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.schurmann_generalised-Union{Tuple{T}, Tuple{CountVector, Vector{T}}} where T&lt;:Real" href="#DiscreteEntropy.schurmann_generalised-Union{Tuple{T}, Tuple{CountVector, Vector{T}}} where T&lt;:Real"><code>DiscreteEntropy.schurmann_generalised</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">schurmann_generalised(data::CountVector, xis::XiVector{T}) where {T&lt;:Real}</code></pre><p><a href="https://arxiv.org/pdf/2111.11175.pdf">schurmann_generalised</a></p><p class="math-container">\[\hat{H}_{\tiny{SHU}} = \psi(N) - \frac{1}{N} \sum_{i=1}^{K} \, h_i \left( \psi(h_i) + (-1)^{h_i} ∫_0^{\frac{1}{\xi_i} - 1} \frac{t^{h_i}-1}{1+t}dt \right)
\]</p><p>Return the generalised Schurmann entropy estimation, given a countvector <code>data</code> and a xivector <code>xis</code>, which must both be the same length.</p><pre><code class="nohighlight hljs">schurmann_generalised(data::CountVector, xis::Distribution, scalar=false)</code></pre><p>Computes the generalised Schurmann entropy estimation, given a countvector <em>data</em> and a distribution <em>xis</em>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L114-L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.shrink-Tuple{CountData}" href="#DiscreteEntropy.shrink-Tuple{CountData}"><code>DiscreteEntropy.shrink</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">shrink(data::CountData)</code></pre><p>Return the Shrinkage, or James-Stein estimator of Shannon entropy for <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{\tiny{SHR}} = - \sum_{i=1}^{K} \hat{p}_x^{\tiny{SHR}} \log(\hat{p}_x^{\tiny{SHR}})\]</p><p>where</p><p class="math-container">\[\hat{p}_x^{\tiny{SHR}} = \lambda t_x + (1 - \lambda) \hat{p}_x^{\tiny{ML}}\]</p><p>and</p><p class="math-container">\[\lambda = \frac{ 1 - \sum_{x=1}^{K} (\hat{p}_x^{\tiny{SHR}})^2}{(n-1) \sum_{x=1}^K (t_x - \hat{p}_x^{\tiny{ML}})^2}\]</p><p>with</p><p class="math-container">\[t_x = 1 / K\]</p><p><strong>Notes</strong></p><p>Based on the implementation in the R package <a href="https://cran.r-project.org/web/packages/entropy/index.html">entropy</a></p><p><strong>External Links</strong></p><p><a href="https://www.jmlr.org/papers/volume10/hausser09a/hausser09a.pdf">Entropy Inference and the James-Stein Estimator</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L255-L284">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.to_bans-Tuple{Float64}" href="#DiscreteEntropy.to_bans-Tuple{Float64}"><code>DiscreteEntropy.to_bans</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">to_bans(x::Float64)</code></pre><p>Return <span>$\frac{h}{log(10)}$</span> where <code>h</code> is in nats</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/utils.jl#L41-L44">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.to_bits-Tuple{Float64}" href="#DiscreteEntropy.to_bits-Tuple{Float64}"><code>DiscreteEntropy.to_bits</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">to_bits(x::Float64)</code></pre><p>Return <span>$\frac{h}{\log(2)}$</span> where h is in nats</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/utils.jl#L33-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.xlogx-Tuple{Any}" href="#DiscreteEntropy.xlogx-Tuple{Any}"><code>DiscreteEntropy.xlogx</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">xlogx(x::Float64)</code></pre><p>Returns <code>x * log(x)</code> for <code>x ≥ 0</code>, or 0.0 if x is zero</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Core/utils.jl#L17-L20">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.zhang-Tuple{CountData}" href="#DiscreteEntropy.zhang-Tuple{CountData}"><code>DiscreteEntropy.zhang</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">zhang(data::CountData)</code></pre><p>Return the Zhang estimate of the Shannon entropy of <code>data</code> in nats.</p><p>The recommended definition of Zhang&#39;s estimator is from <a href="https://www.tandfonline.com/doi/full/10.1080/09296174.2013.830551">Grabchak <em>et al.</em></a></p><p class="math-container">\[\hat{H}_Z = \sum_{i=1}^K \hat{p}_i \sum_{v=1}^{N - h_i} \frac{1}{v} ∏_{j=0}^{v-1} \left( 1 + \frac{1 - h_i}{N - 1 - j} \right)\]</p><p>The actual algorithm comes from <a href="https://arxiv.org/abs/1707.08290">Fast Calculation of entropy with Zhang&#39;s estimator</a> by Lozano <em>et al.</em>.</p><p><strong>Links</strong></p><p><a href="https://dl.acm.org/doi/10.1162/NECO_a_00266">Entropy estimation in turing&#39;s perspective</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/b13f95cbe2ca32f64172dc35d1f00973063b9815/src/Estimators/Frequentist/frequentist.jl#L195-L209">source</a></section></article></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Saturday 30 March 2024 08:13">Saturday 30 March 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
